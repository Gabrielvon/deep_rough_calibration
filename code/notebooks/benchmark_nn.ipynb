{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load up the fully-trained neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import importlib.util\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Important directories\n",
    "code_dir = os.path.dirname(os.getcwd())\n",
    "deep_cal_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Allows to import my own module\n",
    "sys.path.insert(0, code_dir)\n",
    "\n",
    "from ann.neural_network import dense_nn\n",
    "from ann.predict import predict\n",
    "from ann.helpers import load_labeled_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging stuff\n",
    "logger = logging.getLogger(\"benchmarking\")\n",
    "logger.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(deep_cal_dir + \"/code/logs/benchmarking.log\")    \n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't load save_path when it is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1c78b0fdf847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoint loaded. Ready for prediction.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1532\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't load save_path when it is None."
     ]
    }
   ],
   "source": [
    "# NN params\n",
    "layer_sizes = [64]*4\n",
    "nb_features = 7\n",
    "nb_labels = 1\n",
    "\n",
    "# File directory to saved TF checkpoint files\n",
    "ckpt_dir = deep_cal_dir + '/data/heston/nn/'\n",
    "\n",
    "# NN construction\n",
    "logger.info(\"Building computational graph of a fully connected neural network.\")\n",
    "nn = dense_nn(nb_features, layer_sizes, nb_labels)\n",
    "logger.info('Building completed.')\n",
    "\n",
    "# Restoring weights and biases to NN\n",
    "logger.info('Starting interactive tensorflow session.')\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "logger.info('Checkpoint loaded. Ready for prediction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network was trained using standardised input data and so weights and biases reflect the scale and offset of that data. To make predictions on unseen test inputs, we hence need to standardise these inputs using the training mean and standard deviation. This standardisation procedure will be encoded in a utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data configuration\n",
    "train_filename = deep_cal_dir + '/data/heston/training_data.csv'\n",
    "feature_cols = [ _ for _ in range(7)]\n",
    "label_cols = [7]\n",
    "\n",
    "# Extract mean and std from training data\n",
    "train_tuple = load_labeled_csv(train_filename, feature_cols, label_cols)\n",
    "train_mean = np.mean(train_tuple.features, axis=0)\n",
    "train_std = np.std(train_tuple.features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_inputs(test_inputs, train_mean, train_std):\n",
    "    \n",
    "    logger.info(\"Normalizing input labeled data.\")\n",
    "    \n",
    "    test_inputs -= train_mean\n",
    "    test_inputs /= train_std\n",
    "    \n",
    "    logger.info('Finished normalization of input data.')\n",
    "    \n",
    "    return test_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing and accuracy on general test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = deep_cal_dir + '/data/heston/testing_data.csv'\n",
    "\n",
    "# Import and standardisation of test data\n",
    "test_tuple = load_labeled_csv(test_filename, feature_cols, label_cols)\n",
    "test_tuple.features = standardise_inputs(test_tuple.features, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit test_results = predict(test_tuple.features, nn, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def set_style():\n",
    "\n",
    "    sns.set_context(\"paper\")\n",
    "\n",
    "    sns.set(font='serif')\n",
    "    \n",
    "    sns.set_style(\"white\", {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times\", \"Palatino\", \"serif\"]\n",
    "    })\n",
    "    \n",
    "set_style()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = predict(test_tuple.features, nn, sess)\n",
    "absolute_error = np.abs(test_results-test_tuple.labels)\n",
    "relative_error = absolute_error/test_tuple.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_RE_hist(relative_error):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    ax.hist(relative_error, bins=3000, density=True)\n",
    "    ax.set_xlabel('Absolute relative error')\n",
    "    ax.set_ylabel('Frequency density')\n",
    "    ax.xaxis.set_major_formatter(ticker.PercentFormatter(xmax=1))\n",
    "    ax.set_xlim(0, 0.15)\n",
    "    # ax.set_ylim(0, 0.03)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig('test.pdf')\n",
    "\n",
    "draw_RE_hist(relative_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def set_style():\n",
    "\n",
    "    sns.set_context(\"paper\")\n",
    "\n",
    "    sns.set(font='serif')\n",
    "    \n",
    "    sns.set_style(\"white\", {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times\", \"Palatino\", \"serif\"]\n",
    "    })\n",
    "    \n",
    "set_style()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing smile slice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deep_cal_dir + '/data/heston/smile_slice_0.04.csv'\n",
    "# filename = deep_cal_dir + '/data/heston/testing_data.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.moneyness >= 0.75) & (df.moneyness <=1.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = df.values.copy()\n",
    "features = raw_data[:,:7]\n",
    "labels = raw_data[:,7]\n",
    "features = standardise_inputs(features, train_mean, train_std)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictions'] = predict(features, nn, sess)\n",
    "df['absolute_error'] = np.abs(df.predictions-labels)\n",
    "df['relative_error'] = df.absolute_error/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(df.moneyness), df.iv, np.log(df.moneyness), df.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('relative_error', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_pivot_df = df.pivot(index='time to maturity (years)', \n",
    "                           columns='moneyness', values='inv_spread')\n",
    "\n",
    "    ax = sns.heatmap(oi_pivot_df, cmap=\"Blues\", cbar=True, xticklabels=15, yticklabels=2)\n",
    "    ax.invert_yaxis()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
