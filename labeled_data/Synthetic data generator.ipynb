{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data generator for deep calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we generate the synthetic labeled data needed for training the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the graphics environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "def set_style():\n",
    "\n",
    "    sns.set_context(\"paper\")\n",
    "\n",
    "    sns.set(font='serif')\n",
    "    \n",
    "    sns.set_style(\"white\", {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times\", \"Palatino\", \"serif\"]\n",
    "    })\n",
    "\n",
    "set_style()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint distribution of (moneyness,time_to_maturity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation is an expensive and time-consuming task. Since we want our neural network to be as accurate as possible in the parameter region of greatest liquidity, it makes sense to provide more training data in that region by sampling more labeled data from it. Specifically, we will hence compute an empirical joint probability distribution of (moneyness, time_to_maturity) from SPX data where liquidity is proxied by *open interest*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get settlement prices for call and put options on the S&P 500 index from www.cboe.com/DelayedQuote/QuoteTableDownload.aspx ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market parameters\n",
    "spot_price = 2731.25\n",
    "today = pd.to_datetime('20180215', format='%Y%m%d', errors='coerce')\n",
    "nb_trading_days = 365\n",
    "\n",
    "# Read in raw SPX data \n",
    "df = pd.read_csv('raw_data/spx_20180215.csv', skiprows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discard all information about puts and also some columns that are of no interest in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove information about puts\n",
    "df = df.iloc[:,:7]\n",
    "\n",
    "# Drop superfluous columns\n",
    "del_cols = ['Last Sale', 'Net', 'Vol', 'Bid', 'Ask']\n",
    "df.drop(del_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(data):\n",
    "    \n",
    "    # Create a dictionary for conversion of month code (A-L: Jan-Dec for \n",
    "    # call, M-X: Jan-Dec for put) to MM\n",
    "    letters = [letter for letter in string.ascii_uppercase[:24]]\n",
    "    numbers = [nb for nb in range(1, 13)] + [nb for nb in range(1, 13)]\n",
    "    cal_dict = dict(zip(letters,numbers))\n",
    "    \n",
    "    # Identify ticker from content\n",
    "    ticker = str(data).split(' ')[3][1:-1]\n",
    "    \n",
    "    # Is option European or American?\n",
    "    is_euro = True if ticker[-1] == 'E' else False\n",
    "    \n",
    "    # Is option Weekly?\n",
    "    is_weekly = True if ticker[3] == 'W' else False\n",
    "    \n",
    "    # Differentiate between variables ticker lengths for Weekly and \n",
    "    # Standard options\n",
    "    ticker = ticker[4:] if is_weekly else ticker[3:]\n",
    "    \n",
    "    # Detect time to maturity in years from ticker symbol.\n",
    "    yy = '20'+ ticker[0:2]\n",
    "    dd = ticker[2:4]\n",
    "    mm = str(cal_dict[ticker[4]]).zfill(2)\n",
    "    timestamp = pd.to_datetime(yy+mm+dd, format='%Y%m%d', errors='coerce')\n",
    "    dtm = (timestamp - today).days\n",
    "    dtm_frac = dtm/nb_trading_days\n",
    "    \n",
    "    # Detect moneyness from ticker symbol.\n",
    "    strike = ticker[5:9]\n",
    "    cont = True\n",
    "    i = 0\n",
    "\n",
    "    while cont:\n",
    "        cont = strike[i].isdigit()\n",
    "        if cont == False:\n",
    "            strike = strike[:i]\n",
    "        else:\n",
    "            i += 1\n",
    "            if i >= len(strike):\n",
    "                break\n",
    "    \n",
    "    moneyness = spot_price/int(strike)\n",
    "    \n",
    "    return is_euro, is_weekly, dtm_frac, moneyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information from ticker symbol and add to dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from ticker and write to separate df\n",
    "df2 = df['Calls'].apply(extract_info).apply(pd.Series)\n",
    "df2.columns = ['is_euro', 'is_weekly', 'time to maturity (years)', \n",
    "               'moneyness']\n",
    "\n",
    "# Merge two dataframes\n",
    "df = pd.concat([df, df2], axis=1)\n",
    "\n",
    "# Drop the now redundant column 'Calls'\n",
    "df.drop(['Calls'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we only consider European options and SPX Weeklys which expire every Monday, Wednesday and Friday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['is_euro'] == True]\n",
    "df = df[df['is_weekly'] == True]\n",
    "df.drop(['is_euro'], axis=1, inplace=True)\n",
    "df.drop(['is_weekly'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, those options with 0 *Open interest* are of no interest to us, so drop them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Open Int'] != 0]\n",
    "df.sort_values('moneyness', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Possibility to export dataframe to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('raw_data/processed_spx_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot we can see that large parts of the parameter region are empty with no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_pivot_df = df.pivot(index='time to maturity (years)', \n",
    "                       columns='moneyness', values='Open Int')\n",
    "\n",
    "ax = sns.heatmap(oi_pivot_df, cmap=\"Blues\", cbar=True, xticklabels=15, yticklabels=2)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.savefig('raw_data/full_moneyness_ttm_heatmap.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus reduce the parameter region considered to $0.75\\leq moneyness \\leq 1.2$ and $0\\leq time to maturity \\leq 0.25$ and plot another heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['moneyness'] <= 1.2]\n",
    "df = df[df['moneyness'] >= 0.75]\n",
    "df = df[df['time to maturity (years)'] <= 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_pivot_df = df.pivot(index='time to maturity (years)', \n",
    "                       columns='moneyness', values='Open Int')\n",
    "\n",
    "ax = sns.heatmap(oi_pivot_df, xticklabels=10, cmap=\"BuPu\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.savefig('raw_data/reduced_moneyness_ttm_heatmap.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE Estimation of (Moneyness, Time to Maturity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, our goal is to sample from $(moneyness, time to maturity)$ and more crucially, to sample more from parameter regions with higher liquidity as proxied by *Open interest*. The idea is to fit a Kernel Density Estimation to our data using the package ScikitLearn and then generate new samples from it. Before we can do that, we need to preprocess our data however. Since KDE cannot weigh the individual pairs by *Open interest*, the hack is to create a new df where each row is duplicated until the number of rows with that data point is equal to its *Open interest*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexing dataframe so that it starts from 1\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Initiating a new df of appropriate size\n",
    "total_interest = df['Open Int'].sum()\n",
    "kde_df = pd.DataFrame(index = np.arange(0, total_interest), \n",
    "                      columns=['time to maturity (years)', 'moneyness'], \n",
    "                      dtype='float64')\n",
    "\n",
    "# Filling the new df with entries according to their multiplicities \n",
    "# proxied by open interest\n",
    "kde_index = 0\n",
    "\n",
    "for i in df.index:\n",
    "    \n",
    "    mult = df.loc[i, 'Open Int']\n",
    "    values = [df.loc[i, ['time to maturity (years)', 'moneyness']]]*mult\n",
    "    kde_df.loc[kde_index:kde_index + mult-1] = values\n",
    "    \n",
    "    kde_index += mult  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach I : Seaborn visualization\n",
    "Seaborn is a visualization library and includes a KDE Plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Bivariate KDE of moneyness and time to maturity')\n",
    "x = kde_df['moneyness']\n",
    "y = kde_df['time to maturity (years)']\n",
    "ax = sns.kdeplot(x, y, cbar=True, shade=True, shade_lowest=False, cmap='BuPu')\n",
    "fig.savefig(\"sns_kde_money_maturities_V3.pdf\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KDE plot shows what one might expect from the heatmap before. Unfortunately, Seaborn does not return the actual KDE model it computes in its backend, so we also can't sample from it. Digging in the source code reveals that it uses - if installed - the statsmodels KDE estimator. Otherwise it defaults to the scipy KDE estimator. So our next try will be to use the statsmodel implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach II: Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.kernel_density import KDEMultivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = kde_df[['moneyness', 'time to maturity (years)']]\n",
    "kde = KDEMultivariate(X, 'cc', bw='normal_reference')\n",
    "kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is very fast but it lacks two important features:\n",
    "1. There is no method to sample from the distribution.\n",
    "2. There is no chance to include finite bounds for $(K,T)$ which is especially bad for $T$ as the KDE would span over the negative domain as well, losing probability mass over a nonsensible domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach III: Scikit-Learn KDE\n",
    "Unlike statsmodels, this KDE estimator does have a method to sample from but just like statsmodels there is no possibility to include bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statsmodels.nonparametric.api as smnp\n",
    "\n",
    "data = kde_df[['moneyness', 'time to maturity (years)']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two choices for bandwidth selection. Either Scott's rule or Cross validation. Run only one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scott's rule (rule of thumb) for bandwidth selection\n",
    "bw_x = smnp.bandwidths.bw_scott(df['moneyness'])\n",
    "bw_y = smnp.bandwidths.bw_scott(df['time to maturity (years)'])\n",
    "\n",
    "print('bw_x: {}, bw_y: {}'.format(bw_x, bw_y))\n",
    "\n",
    "kde = KernelDensity(bandwidth=max(bw_x, bw_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grid search cross-validation to optimize the bandwidth\n",
    "params = {'bandwidth': np.logspace(-3, -1, 5)}\n",
    "grid = GridSearchCV(KernelDensity(), params, verbose=sys.maxsize, n_jobs=-1)\n",
    "grid.fit(data)\n",
    "\n",
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "\n",
    "kde = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run KDE Estimation of Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the KDE extends to regions outside the input region we have previously considered. That's why we will use rejection sampling below to compute our new samples.\n",
    "Remember we want $0.75\\leq moneyness \\leq 1.2$ and $0\\leq time to maturity \\leq 0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 10**6\n",
    "new_data = pd.DataFrame(index=np.arange(10**6),\n",
    "                        columns=['moneyness', 'time to maturity (years)'],\n",
    "                        dtype='float64')\n",
    "\n",
    "valid_counter = 0\n",
    "\n",
    "while valid_counter < nb_samples:\n",
    "    \n",
    "    rem = nb_samples - valid_counter\n",
    "    \n",
    "    # Generate new samples from estimated probability density.\n",
    "    raw = kde.sample(rem)\n",
    "    \n",
    "    # Identify valid samples\n",
    "    is_valid = (raw[:,0]>0.75) & (raw[:,0]<1.2) & (raw[:,1]>0) & (raw[:,1]<0.25)\n",
    "    valid_samples = raw[is_valid]\n",
    "    nb_valid_samples = valid_samples.shape[0]\n",
    "    \n",
    "    # Writing to df\n",
    "    new_data.loc[valid_counter: valid_counter + nb_valid_samples - 1 ,:] = valid_samples\n",
    "\n",
    "    valid_counter += nb_valid_samples\n",
    "    \n",
    "# Save (moneyness,time to maturity (years)) to disk\n",
    "filepath = 'raw_data/money_maturities.csv'\n",
    "new_data.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic generation of BS Implied Volatility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricer(flag, spot_price, strike, time_to_maturity, vol, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Computes the Black-Scholes price of a European option with possibly\n",
    "    vectorized inputs.\n",
    "\n",
    "    :param flag: either \"c\" for calls or \"p\" for puts\n",
    "    :param spot_price: spot price of the underlying\n",
    "    :param strike: strike price\n",
    "    :param time_to_maturity: time to maturity in years\n",
    "    :param vol: annualized volatility assumed constant until expiration\n",
    "    :param risk_free_rate: risk-free interest rate\n",
    "\n",
    "    :type flag: string\n",
    "    :type spot_price: float / numpy array\n",
    "    :type strike: float / numpy array\n",
    "    :type time_to_maturity: float / numpy array\n",
    "    :type vol: float / numpy array\n",
    "    :type risk_free_rate: float\n",
    "\n",
    "    :return: Black-Scholes price\n",
    "    :rtype: float / numpy array\n",
    "\n",
    "    # Example taking vectors as inputs\n",
    "\n",
    "    >>> spot = np.array([3.9,4.1])\n",
    "    >>> strike = 4\n",
    "    >>> time_to_maturity = 1\n",
    "    >>> vol = np.array([0.1, 0.2])\n",
    "    >>> rate = 0\n",
    "\n",
    "    >>> p = BlackScholes.pricer('c', spot, strike, time_to_maturity, vol, rate)\n",
    "    >>> expected_price = np.array([0.1125, 0.3751])\n",
    "\n",
    "\n",
    "    >>> abs(expected_price - p) < 0.0001\n",
    "    array([ True,  True], dtype=bool)\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename variables for convenience.\n",
    "\n",
    "    S = spot_price\n",
    "    K = strike\n",
    "    T = time_to_maturity\n",
    "    r = risk_free_rate\n",
    "\n",
    "    # Compute option price.\n",
    "\n",
    "    d1 = 1/(vol * np.sqrt(T)) * (np.log(S) - np.log(K) + (r+0.5 * vol**2) * T)\n",
    "\n",
    "    d2 = d1 - vol * np.sqrt(T)\n",
    "\n",
    "    if flag == 'c':\n",
    "\n",
    "        price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "    elif flag == 'p':\n",
    "\n",
    "        price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "\n",
    "    return price\n",
    "\n",
    "def write_to_csv(file_name, data, header):\n",
    "\n",
    "        np.savetxt(file_name, data, delimiter=',', newline='\\n', header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate our labeled data. Note that the number of samples of the precomputed bivariate data of (moneyness, time to maturity) has to equal the number of desired samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare seed for sampling from parameter regions.\n",
    "np.random.seed()\n",
    "\n",
    "nb_samples = 10**6\n",
    "\n",
    "# Model parameters\n",
    "spot = 1           # spot\n",
    "rate = 0           # rate\n",
    "flag = 'c'      # 'c' for call\n",
    "vol_lower = 0.01\n",
    "vol_higher = 1\n",
    "\n",
    "# Reading and transforming precomputed data\n",
    "input_data = np.load('money_maturities.npy')\n",
    "moneyness, maturities = input_data[:,0], input_data[:,1]\n",
    "strikes = spot/moneyness\n",
    "\n",
    "# Sample uniformly from input space.\n",
    "vols = np.random.uniform(vol_lower, vol_higher, nb_samples)\n",
    "\n",
    "# Compute Black-Scholes price. \n",
    "prices = pricer(flag, spot, strikes, maturities, vols, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labeled pair and randomly permute rows\n",
    "labeled_data = np.stack([moneyness, maturities, prices, vols], axis=1)\n",
    "labeled_data = np.random.permutation(labeled_data)\n",
    "\n",
    "# Dissecting labeled pairs into training, validation and testing sets.\n",
    "training_data = labeled_data[:8*10**5,:]\n",
    "validation_data = labeled_data[8*10**5:9*10**5,:]\n",
    "testing_data = labeled_data[9*10**5:, :]\n",
    "print('Shapes: train {}, valid {}, test {}'.format(training_data.shape, validation_data.shape,\n",
    "                                                  testing_data.shape))\n",
    "\n",
    "# Write labeled data to .csv file.\n",
    "header = 'moneyness, maturities, prices, vols'\n",
    "write_to_csv('training_data_new.csv', training_data, header)\n",
    "write_to_csv('validation_data_new.csv', validation_data, header)\n",
    "write_to_csv('testing_data_new.csv', testing_data, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic generation of Heston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import numba\n",
    "from py_vollib.black_scholes.implied_volatility import implied_volatility\n",
    "from py_lets_be_rational.exceptions import BelowIntrinsicException\n",
    "import sklearn.utils\n",
    "\n",
    "r = robjects.r\n",
    "r.source(\"heston.R\")\n",
    "r_pricer = r('HestonCallClosedForm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heston_pricer(lambd, vbar, eta, rho, v0, r, tau, S0, K):\n",
    "    \"\"\"\n",
    "    Computes European Call price under Heston dynamics with closedform solution.\n",
    "\n",
    "    :param lambd: mean-reversion speed\n",
    "    :param vbar: long-term average volatility\n",
    "    :param eta: volatility of vol\n",
    "    :param rho: correlation between stock and vol\n",
    "    :param v0: intial volatility\n",
    "    :param r: risk-free interest rate\n",
    "    :param tau: time to maturity (year = 365 days)\n",
    "    :param S0: initial share price\n",
    "    :param K: strike price\n",
    "\n",
    "    :return: Heston price, Black-Scholes implied volatility\n",
    "    :rtype: float, float\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        price = r_pricer(lambd, vbar, eta, rho, v0, r, tau, S0, K)[0]\n",
    "\n",
    "        try:\n",
    "\n",
    "            iv = implied_volatility(price, S0, K, tau, r, 'c')\n",
    "\n",
    "        except BelowIntrinsicException:\n",
    "\n",
    "            print('Below Intrinsic Exception with parameters:', lambd, vbar, \n",
    "                   eta, rho, v0, r, tau, S0, K)\n",
    "\n",
    "            iv = None\n",
    "\n",
    "    except rpy2.rinterface.RRuntimeError:\n",
    "\n",
    "        print('R Runtime Error with parameters:', lambd, vbar, eta, rho, \n",
    "              v0, r, tau, S0, K)\n",
    "\n",
    "        price, iv = None, None\n",
    "\n",
    "    return price, iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare seed for sampling from parameter regions.\n",
    "np.random.seed()\n",
    "\n",
    "nb_samples = 10**6\n",
    "\n",
    "# Standardised parameters\n",
    "S0 = 1\n",
    "r = 0\n",
    "\n",
    "# Heston parameter bounds by Moodley (2005)\n",
    "lambd_bounds = [0, 10]\n",
    "vbar_bounds = [0, 1]\n",
    "eta_bounds = [0, 5]\n",
    "rho_bounds = [-1, 0]\n",
    "v0_bounds = [0, 1]\n",
    "\n",
    "# Import and initialising of data\n",
    "df = pd.read_csv('raw_data/money_maturities.csv')\n",
    "moneyness = df['moneyness'].values\n",
    "maturity = df['time to maturity (years)'].values\n",
    "strike = S0/moneyness\n",
    "\n",
    "columns = ['lambda', 'vbar', 'eta', 'rho', 'v0', 'maturity', 'moneyness', 'iv']\n",
    "df = pd.DataFrame(np.zeros((nb_samples,8)), columns=columns)\n",
    "\n",
    "# Counter that tracks the amount of computed labeled pairs\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation of Heston labeled pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"heston_errors.csv\", \"w\") as log:\n",
    "\n",
    "    log.write(\"lambda, vbar, eta, rho, v0, maturity, moneyness, price, \\\n",
    "               intrinsic, price < intrinsic, abs(price) < 1E-5, OK?  \\n\" )\n",
    "    \n",
    "    while count < nb_samples:\n",
    "\n",
    "        # Sample uniformly from Heston parameter space.\n",
    "        lambd = np.random.uniform(lambd_bounds[0], lambd_bounds[1])\n",
    "        vbar = np.random.uniform(vbar_bounds[0], vbar_bounds[1])\n",
    "        eta = np.random.uniform(eta_bounds[0], eta_bounds[1])\n",
    "        rho = np.random.uniform(rho_bounds[0], rho_bounds[1])\n",
    "        v0 = np.random.uniform(v0_bounds[0], v0_bounds[1])\n",
    "\n",
    "        # Take respective (strike, maturity) pair from precomputed data.\n",
    "        K = strike[count]\n",
    "        T = maturity[count]\n",
    "        M = moneyness[count]\n",
    "\n",
    "        # Calculate Black-Scholes implied vol from Heston price.\n",
    "        price, iv = heston_pricer(lambd, vbar, eta, rho, v0, r, T, S0, K)\n",
    "        \n",
    "        # Running through possible cases returned by heston_pricer.\n",
    "        if iv is not None:\n",
    "            \n",
    "            df.loc[count, columns] = [lambd, vbar, eta, rho, v0, T, M, iv]\n",
    "            \n",
    "            print('Count {}/{}'.format(count + 1, nb_samples))\n",
    "\n",
    "            # Increase running counter.\n",
    "            count += 1\n",
    "            \n",
    "        elif price is not None:\n",
    "\n",
    "            # Collect all necessary information to judge why it failed.\n",
    "            error_data = (lambd, vbar, eta, rho, v0, T, M, \n",
    "                          price, np.max(S0 - K, 0), price < np.max(S0 - K), \n",
    "                          np.abs(price) < 1E-5, \n",
    "                          (price < np.max(S0 - K)) or (np.abs(price) < 1E-5))\n",
    "\n",
    "            log.write(\"%s \\n\" % repr(error_data))\n",
    "            \n",
    "        else:\n",
    "\n",
    "            error_data = (lambd, vbar, eta, rho, v0, T, M)\n",
    "\n",
    "            log.write(\"%s \\n\" % repr(error_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sklearn.utils.shuffle(df)\n",
    "\n",
    "# Dissecting labeled pairs into training, validation and testing sets.\n",
    "train_df, validation_df, test_df = np.split(df, [8*10**5, 9*10**5], axis=0)\n",
    "\n",
    "print('Shapes: \\n train {}, validation {}, test {}'.format(train_df.shape, validation_df.shape,\n",
    "                                                  test_df.shape))\n",
    "\n",
    "# Write labeled data to .csv file.\n",
    "train_df.to_csv('heston/training_data.csv', index=False)\n",
    "validation_df.to_csv('heston/validation_data.csv', index=False)\n",
    "test_df.to_csv('heston/testing_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
